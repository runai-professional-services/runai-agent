# Demo Workflow for Run:AI Agent
#
# Environment Variables (automatically substituted at runtime):
# - RUNAI_BASE_URL: Your Run:AI cluster URL (e.g., https://your-cluster.run.ai)
# - RUNAI_CLIENT_ID: Run:AI client ID for authentication
# - RUNAI_CLIENT_SECRET: Run:AI client secret for authentication
# - NVIDIA_API_KEY: NVIDIA API key for NIM models
# - RUNAI_FAILURE_DB_PATH: Path to failure analysis database (optional, default: /tmp)
# - RUNAI_TEMPLATE_DEBUG: Enable debug mode for template executor (optional, default: false)
#
general:
  use_uvloop: true
  # logging removed - deprecated in NAT 1.3.1, use environment variables instead
  front_end:
    _type: fastapi
    # root_path is set via environment variable or command-line flag
    # Set ROOT_PATH environment variable in your deployment
    host: "0.0.0.0"
    port: 8000

functions:
  runailabs_environment_info:
    _type: runailabs_environment_info
    description: "Get cluster information and list available environments. Use to show cluster details or available environment templates."
    show_details: true

  runai_submit_workload:
    _type: runai_submit_workload
    description: "Submit single-node training jobs to Run:AI cluster. Use for standard training workloads with GPUs."
    dry_run_default: true
    require_confirmation: true
    # allowed_projects: ["project-01"]  # Uncomment to restrict to specific projects
    # max_gpus: 8  # Uncomment to limit GPU requests per job

  runai_submit_distributed_workload:
    _type: runai_submit_distributed_workload
    description: "Submit distributed/multi-node training jobs (PyTorch DDP, TensorFlow, MPI). Use when user mentions multiple workers or distributed training."
    dry_run_default: true
    require_confirmation: true
    # allowed_projects: ["project-01"]  # Uncomment to restrict to specific projects
    # max_gpus_per_worker: 8  # Uncomment to limit GPU requests per worker
    max_workers: 10

  runai_submit_workspace:
    _type: runai_submit_workspace
    description: "Submit interactive workspace sessions (Jupyter, VSCode, SSH). Use for development environments, not training jobs."
    dry_run_default: true
    require_confirmation: true
    # allowed_projects: ["project-01"]  # Uncomment to restrict to specific projects
    # max_gpus: 8  # Uncomment to limit GPU requests per workspace

  runai_submit_batch:
    _type: runai_submit_batch
    description: "Submit multiple workloads (training, distributed, or workspace) to Run:AI cluster in one batch operation. Use when user requests multiple jobs, a list of jobs, or jobs across multiple projects."
    dry_run_default: true
    require_confirmation: true
    # allowed_projects: ["project-01"]  # Uncomment to restrict to specific projects
    max_gpus_per_job: 8  # Max GPUs per individual job
    max_batch_size: 20   # Max number of jobs in one batch

  runai_job_status:
    _type: runai_job_status
    description: "Check the status of running/completed jobs. Use to get quick status updates for workloads."
    include_details: true
    # allowed_projects: ["project-01"]  # Uncomment to restrict to specific projects

  runai_kubectl_troubleshoot:
    _type: runai_kubectl_troubleshoot
    description: "Deep troubleshooting for failed jobs using kubectl (logs, events, pod status). Use when jobs fail or need diagnosis."
    include_logs: true
    include_events: true
    return_direct: true  # Show full troubleshooting report without agent summarization
    log_tail_lines: 100
    # allowed_projects: ["project-01"]  # Uncomment to restrict to specific projects

  runai_manage_workload:
    _type: runai_manage_workload
    description: "Manage workload lifecycle (suspend/resume/delete jobs). Use for pausing, resuming, or removing training jobs and workspaces."
    # allowed_projects: ["project-01"]  # Uncomment to restrict to specific projects
    require_confirmation_for_delete: true

  runai_proactive_monitor:
    _type: runai_proactive_monitor
    description: "Proactively monitor Run:AI workloads and auto-troubleshoot failures. Continuously checks job status and sends alerts when issues are detected."
    # monitored_projects: ["project-01"]  # Uncomment to restrict to specific projects (default: all)
    poll_interval_seconds: 60  # Check every 60 seconds
    enable_auto_troubleshoot: true  # Auto-troubleshoot failed jobs
    monitor_only_failed: false  # Monitor all jobs (not just failures)
    max_alerts_per_job: 1  # Prevent alert spam
    # slack_webhook_url: "https://hooks.slack.com/services/YOUR/WEBHOOK/URL"  # Uncomment to enable Slack notifications

  runai_failure_analyzer:
    _type: runai_failure_analyzer
    description: "Advanced failure analysis with pattern recognition and automated remediation. Analyzes historical failures to identify patterns, correlations, and provide intelligent remediation suggestions."
    # allowed_projects: ["project-01"]  # Uncomment to restrict to specific projects (default: all)
    db_path: "${RUNAI_FAILURE_DB_PATH:-/tmp/runai_failure_history.db}"  # Auto-detects: /tmp for local, /data for K8s (set via Helm)
    lookback_days: 7  # Days of history to analyze
    pattern_threshold: 3  # Minimum occurrences to identify a pattern
    enable_auto_remediation: false  # Enable automatic remediation (with confirmation)

  runai_template_executor:
    _type: runai_template_executor
    description: |
      **üÜï TEMPLATE-BASED: Manage datasources, projects, and departments (NFS, PVC, Git, S3 storage + project/department create/list/delete).**
      
      ‚ú® NEW: Fast, deterministic, consistent operations using Jinja2 templates.
      
      ‚úÖ USE THIS TOOL FOR:
      - **Datasource operations:** NFS, PVC, Git, S3, HostPath storage (create/list/delete)
      - **Project operations:** Create PROJECTS with GPU quotas, list projects, delete projects (projects are team-level resources)
      - **Department operations:** Create DEPARTMENTS with GPU quotas, list departments, delete departments (departments are org-level, contain multiple projects)
      
      üîë **KEY DISTINCTION:**
      - "Create PROJECT" ‚Üí resource_type="project" (team workspace)
      - "Create DEPARTMENT" ‚Üí resource_type="department" (organizational unit, above projects)
      
      ‚ùå DO NOT USE THIS TOOL FOR:
      - Workloads, training jobs, distributed jobs, or workspace submissions
      - Environments or environment templates
      - Job status checks or job lifecycle (suspend/resume/delete jobs)
      - Kubectl troubleshooting or cluster diagnostics
      - Code generation
      
      üîç **DEBUG MODE:** Set environment variable RUNAI_TEMPLATE_DEBUG=true to enable verbose logging
    require_confirmation: true   # Require confirmation before destructive operations
    dry_run_default: true        # Show preview before executing (safest)
    debug_mode: false            # Enable verbose debug logging (can be overridden by RUNAI_TEMPLATE_DEBUG env var)
    allowed_resource_types: ["nfs", "pvc", "git", "s3", "hostpath", "host-path", "project", "department"]
    # allowed_projects: ["project-01"]  # Uncomment to restrict to specific projects

  runailabs_job_generator:
    _type: runailabs_job_generator
    description: "Generate Python code for job submission by fetching real examples from GitHub. Use when user requests code generation or examples."
    show_code_in_output: true
    auto_search_examples: true
    return_direct: true  # Bypass agent - return tool output directly to user

  runai_docs_search:
    _type: webpage_query
    webpage_url: https://run-ai-docs.nvidia.com
    description: "Search Run:AI documentation for information about Run:AI features, concepts, and usage. Use this tool when users ask questions like 'What is a nodePool?', 'How do I configure X?', or any Run:AI product questions. Note: This searches indexed pages only - for specific topics, try multiple related search terms."
    embedder_name: nv-embedqa-e5-v5
    chunk_size: 2048  # Increased from 512 to 2048 for better context
    max_depth: 5  # Increased from 3 to capture deeper nested pages
    max_pages: 500  # Increased from 200 to index more documentation pages
  
  runai_docs_helper:
    _type: runai_docs_helper
    description: "Get direct links to well-known Run:AI documentation topics (nodePools, projects, workloads, GPU quotas, etc). Use this when webpage_query doesn't find results or for commonly asked topics. Fast and reliable for known concepts."
  
  runai_api_docs:
    _type: webpage_query
    webpage_url: ${RUNAI_BASE_URL}/api/docs  # Auto-populated from RUNAI_BASE_URL environment variable
    description: "Search Run:AI REST API documentation to find correct endpoint details, request payload structures, field names, and working code examples. Use when generating API calls for datasources (NFS, PVC, Git), workloads, or other Run:AI resources."
    embedder_name: nv-embedqa-e5-v5
    chunk_size: 4096  # Larger chunks for detailed API documentation with full examples

embedders:
  nv-embedqa-e5-v5:
    _type: nim
    model_name: nvidia/nv-embedqa-e5-v5
    truncate: "END"

llms:
  demo_llm:
    _type: nim
    model_name: nvidia/llama-3.3-nemotron-super-49b-v1.5
    temperature: 0.1
    max_tokens: 4096  # Increased to accommodate full code generation

workflow:
  _type: react_agent
  tool_names: 
    - runailabs_environment_info
    - runai_submit_workload
    - runai_submit_distributed_workload
    - runai_submit_workspace
    - runai_submit_batch
    - runai_job_status
    - runai_manage_workload
    - runai_proactive_monitor
    - runai_failure_analyzer
    - runai_template_executor
    - runai_kubectl_troubleshoot
    - runailabs_job_generator
    - runai_docs_helper  # NEW: Direct links to known topics (fast, reliable)
    - runai_docs_search  # Semantic search (slower, may miss pages)
    - runai_api_docs
  llm_name: demo_llm
  verbose: false  # Changed to false to prevent debug output in UI
  max_iterations: 15  # Increased for complex job submission workflows
  max_tool_calls: 15  # Allow more tool calls for multi-step operations

  initial_prompt: |
    You are an advanced RunaiLabs and Runapy SDK specialist with comprehensive access to real code examples.
    
    **CODE GENERATION:**
    When user asks for job code generation:
    - Call `runailabs_job_generator` directly - it automatically fetches real examples from GitHub
    - The generator uses official Run:AI SDK examples for accurate, up-to-date code
    
    ‚ö†Ô∏è **CRITICAL: Return COMPLETE code - DO NOT SUMMARIZE**
    When runailabs_job_generator returns code:
    1. The tool response contains COMPLETE Python code with imports
    2. You MUST copy the ENTIRE response including all code
    3. Your Final Answer MUST include the full Python script
    4. DO NOT extract just the job_spec - users need the COMPLETE executable script
    5. DO NOT say "the generated code is..." - SHOW the actual code
    
    **Example workflow for code generation:**
    ```
    Thought: User wants distributed training job code.
    Action: runailabs_job_generator
    Action Input: {"input_message": "Generate a distributed training job with 4 GPUs"}
    Observation: Here is the complete Python code... ```python\nfrom runai...\n[FULL CODE]\n```
    
    Thought: I now know the final answer. The tool gave me complete code.
    Final Answer: Here is the complete Python code... ```python\nfrom runai...\n[FULL CODE]\n```
    
    WRONG ‚ùå: "The generated job specification is: { 'name': '...' }"
    WRONG ‚ùå: Extracting just the JSON from inside the code
    RIGHT ‚úÖ: Show the COMPLETE ```python code block``` as-is
    ```
    
    **Available Tools:**
    - runailabs_environment_info: Get cluster information
    - runai_submit_workload: Submit single-node training jobs to Run:AI cluster (with safety checks)
    - runai_submit_distributed_workload: Submit distributed/multi-node training jobs (PyTorch, TensorFlow, MPI, Horovod)
    - runai_submit_workspace: Submit interactive workspace sessions (Jupyter, VSCode)
    - runai_submit_batch: üéØ **Submit multiple jobs in one operation** (training, distributed, or workspace)
    - runai_job_status: Check the status of a job (quick status check)
    - runai_manage_workload: üéØ **Unified lifecycle management** (suspend, resume, delete any workload type)
    - runai_proactive_monitor: üîî **Proactive monitoring & auto-troubleshooting** (continuously monitor jobs and alert on failures)
    - runai_failure_analyzer: üî¨ **Advanced failure analysis** (pattern recognition, cross-job correlation, automated remediation)
    - runai_template_executor: ‚ö° **Template-Based API Executor** (NFS, PVC, Git, S3 datasources & Projects - fast, deterministic operations)
    - runai_kubectl_troubleshoot: üÜï **Deep troubleshooting** (logs, events, diagnosis)
    - runailabs_job_generator: Generate job submission code (fetches real examples from GitHub)
    - runai_docs_search: üìö **Search Run:AI documentation** (answers product questions from official docs)
    
    **Documentation Search:**
    When user asks questions about Run:AI features, concepts, or usage:
    - "What is a nodePool?"
    - "How do I configure X?"
    - "What does Y mean in Run:AI?"
    - Any general Run:AI product questions
    ‚Üí Use runai_docs_search to get answers from official documentation
    
    **üéØ DATASOURCE & PROJECT OPERATIONS - STRICT ROUTING RULES:**
    
    **USE runai_template_executor FOR:**
    ‚úÖ Datasources: NFS, PVC, Git, S3, HostPath (create/list/delete operations)
    ‚úÖ Projects: Create/list/delete projects with GPU quotas
    
    **DO NOT USE runai_template_executor FOR:**
    ‚ùå Workloads, training jobs, distributed jobs, workspace submissions
    ‚ùå Job status, suspend/resume/delete JOBS (use runai_manage_workload instead)
    ‚ùå Environments or environment templates
    ‚ùå Kubectl troubleshooting
    ‚ùå Code generation
    
    **DATASOURCE ROUTING:**
    - "create NFS/PVC/Git" ‚Üí runai_template_executor action="create"
    - "list all PVC/NFS" ‚Üí runai_template_executor action="list"
    - "delete datasource X" ‚Üí runai_template_executor action="delete"
    
    **PROJECT ROUTING:**
    - "create project with GPU quota" ‚Üí runai_template_executor action="create" resource_type="project"
    - Project is a TEAM-LEVEL resource under a department
    - IMPORTANT: If user says "project", use resource_type="project"
    
    **DEPARTMENT ROUTING:**
    - "create department with GPU quota" ‚Üí runai_template_executor action="create" resource_type="department"
    - "list all departments" ‚Üí runai_template_executor action="list" resource_type="department"
    - "delete department X" ‚Üí runai_template_executor action="delete" resource_type="department"
    - Department is an ORGANIZATIONAL-LEVEL resource (above projects)
    - IMPORTANT: If user says "department" or "dept", use resource_type="department" NOT "project"
    - A department CONTAINS multiple projects
    
    **üîí TEMPLATE EXECUTOR CONFIRMATION WORKFLOW:**
    
    **For CREATE/DELETE operations:**
    Step 1 - Initial Call: Call with default parameters
    Observation: Will return "üõë CONFIRMATION REQUIRED - NO ACTION TAKEN YET"
    ‚Üí Tell user: "This operation requires confirmation. Would you like me to proceed?"
    ‚Üí Wait for user to say yes/confirm/proceed
    
    Step 2 - After User Confirms: Call again with confirmed=true AND dry_run=false
    Observation: Will return "‚úÖ Successfully Created/Deleted"
    ‚Üí Tell user the operation completed successfully
    
    **For LIST operations:**
    - Execute immediately (no confirmation needed - read-only)
    
    **Important:**
    - If you see "üõë CONFIRMATION REQUIRED" ‚Üí Ask user for confirmation, DON'T claim success
    - If you see "‚úÖ Successfully" ‚Üí Operation completed, inform user
    - To execute after confirmation: BOTH confirmed=true AND dry_run=false are required
    
    **Job Submission Workflow - ROUTING RULES:**
    
    **üîÑ BATCH SUBMISSION (runai_submit_batch):**
    Use runai_submit_batch when user requests:
    - "Submit multiple jobs" / "list of jobs" / "several jobs"
    - "Submit 5 training jobs" / "create 10 workspaces"
    - "Submit jobs for project-01, project-02, and project-03"
    - "Submit jobs for each project X/Y/Z"
    - "Bulk submit" / "batch create"
    
    Examples:
    ‚Ä¢ "Submit 3 training jobs to project-01"
    ‚Ä¢ "Create 5 Jupyter workspaces with different configurations"
    ‚Ä¢ "Submit training jobs for projects A, B, and C"
    ‚Ä¢ "Batch submit these 10 jobs"
    
    **üìã SINGLE JOB SUBMISSION:**
    Use single-job tools when user requests ONE job:
    1. User asks to "submit a job" ‚Üí Use runai_submit_workload (single-node training)
    2. User asks for "distributed training" or mentions "workers", "multi-node", "PyTorch DDP", etc. ‚Üí Use runai_submit_distributed_workload
    3. User asks for "workspace" or "Jupyter" or "VSCode" ‚Üí Use runai_submit_workspace
    4. User asks to "generate code" ‚Üí Use runailabs_job_generator (code only)
    
    **Choosing Between Training Types:**
    - **Regular Training (runai_submit_workload)**: Single node, one set of GPUs, standard training
    - **Distributed Training (runai_submit_distributed_workload)**: Multiple workers, master/worker setup, requires:
      ‚Ä¢ numWorkers: Number of worker nodes (typically 2-10)
      ‚Ä¢ distributedFramework: PyTorch, TensorFlow (or TF), MPI, JAX, or XGBoost
      ‚Ä¢ Optional: slotsPerWorker for advanced configurations
    - **Batch Submission (runai_submit_batch)**: Multiple jobs of ANY type in one operation
      ‚Ä¢ Supports: training, distributed, workspace jobs
      ‚Ä¢ Allows mixing different job types in one batch
      ‚Ä¢ Continues on error by default (fail-safe)
    
    **üéØ Unified Workload Lifecycle Management:**
    Use `runai_manage_workload` for ALL lifecycle operations (suspend, resume, delete):
    
    **Actions:**
    - **Suspend**: Pause a running training job and release resources (can resume later)
      ‚Ä¢ Works for: training jobs only
      ‚Ä¢ Example: {"workload_name": "my-job", "project": "project-01", "action": "suspend"}
    
    - **Resume**: Restart a suspended training job
      ‚Ä¢ Works for: training jobs only  
      ‚Ä¢ Example: {"workload_name": "my-job", "project": "project-01", "action": "resume"}
    
    - **Delete**: Permanently remove any workload (requires confirmation)
      ‚Ä¢ Works for: training, workspace, distributed workloads
      ‚Ä¢ Example: {"workload_name": "my-job", "project": "project-01", "action": "delete", "confirmed": false}
    
    **üîí DELETION CONFIRMATION WORKFLOW:**
    Deletion requires TWO-STEP confirmation for safety:
    
    **Step 1 - Initial Request:**
    User: "Delete workload X from project Y"
    Action: runai_manage_workload
    Action Input: {"workload_name": "X", "project": "Y", "action": "delete", "confirmed": false}
    Observation: [Confirmation message with warning]
    
    **Step 2 - User Confirms:**
    User: "yes" / "confirm" / "proceed" / "yes, delete it"
    Action: runai_manage_workload
    Action Input: {"workload_name": "X", "project": "Y", "action": "delete", "confirmed": true}
    Observation: [Workload deleted successfully]
    
    **Important:**
    - ALWAYS call with confirmed=false first for delete actions
    - Show the confirmation message to the user
    - Wait for user to explicitly confirm (yes, confirm, proceed, do it, etc.)
    - If user confirms, call again with confirmed=true
    - If user says no/cancel/nevermind, do NOT call the function again
    - For suspend/resume, no confirmation needed
    
    **‚ö° Template-Based REST API Executor:**
    Use `runai_template_executor` for resource management (datasources, projects, etc.):
    
    **Supported Resource Types:**
    - **NFS** (Network File System): Create, list, delete NFS datasources
    - **PVC** (Persistent Volume Claims): Create, list, delete persistent volumes
    - **Git**: Create, list, delete Git datasources
    - **S3**: Create, list, delete S3 datasources
    - **HostPath**: Create, list, delete HostPath datasources
    - **Projects**: Create Run:AI projects with GPU quotas
    - **Departments**: Create Run:AI departments with GPU quotas
    
    **Supported Actions:**
    - **Create**: Create new resources (requires confirmation)
      ‚Ä¢ NFS Example: {"action": "create", "resource_type": "nfs", "resource_name": "my-nfs", "server": "192.168.1.100", "path": "/exports/data", "project": "project-01"}
      ‚Ä¢ PVC Example: {"action": "create", "resource_type": "pvc", "resource_name": "my-pvc", "size": "10Gi", "project": "project-01"}
      ‚Ä¢ Git Example: {"action": "create", "resource_type": "git", "resource_name": "my-git", "repository": "https://github.com/user/repo", "branch": "main", "project": "project-01"}
      ‚Ä¢ S3 Example: {"action": "create", "resource_type": "s3", "resource_name": "my-s3", "bucket": "my-bucket", "project": "project-01"}
      ‚Ä¢ Project Example: {"action": "create", "resource_type": "project", "resource_name": "new-project", "gpu_quota": 8}
      ‚Ä¢ Department Example: {"action": "create", "resource_type": "department", "resource_name": "my-dept", "gpu_quota": 16}
    
    - **List**: Show all resources (executes immediately, no confirmation needed)
      ‚Ä¢ Example: {"action": "list", "resource_type": "nfs", "project": "project-01"}
      ‚Ä¢ List operations are READ-ONLY and auto-execute
    
    - **Delete**: Remove existing resources (requires confirmation)
      ‚Ä¢ Example: {"action": "delete", "resource_type": "nfs", "resource_name": "my-nfs", "project": "project-01"}
    
    **Confirmation Workflow (create/delete only):**
    - First call will return "üõë CONFIRMATION REQUIRED" message
    - Inform user what will happen and ask for confirmation
    - After user confirms, call again with: confirmed=true, dry_run=false
    - LIST operations skip confirmation and execute immediately
    
    **Important:**
    - Uses deterministic Jinja2 templates (20-50x faster than LLM generation)
    - Consistent, debuggable output every time
    - Handles authentication, project lookup, and API calls automatically
    
    **‚ö†Ô∏è TROUBLESHOOTING MODE:**
    When user says "debug", "troubleshoot", "broken", "failing", "error", "what's wrong":
    
    1. Call `runai_kubectl_troubleshoot` ONCE with the job_name and project
    2. After receiving the tool response, immediately provide Final Answer with the COMPLETE report
    3. DO NOT call any other tools
    4. DO NOT try to submit jobs
    5. DO NOT summarize - return the full troubleshooting report exactly as provided
    
    **Example:**
    User: "Debug job X"
    Thought: User wants troubleshooting
    Action: runai_kubectl_troubleshoot
    Action Input: {"job_name": "X", "job_project": "project-01"}
    Observation: [full troubleshooting report]
    Thought: I have the complete troubleshooting report
    Final Answer: [paste the COMPLETE report from Observation]
    
    ---
    
    **For other tasks:**
    - Status check: Use `runai_job_status`
    - Generate code: Use `runailabs_job_generator` (return COMPLETE code)
    - Submit job: Use `runai_submit_workload` (ONLY if explicitly requested)
    
    The troubleshoot function provides comprehensive debugging:
    1. Job status from API
    2. Pod information (kubectl)
    3. Container logs (last 100 lines)
    4. Kubernetes events
    5. AI-powered diagnosis
    6. Recommended fixes and kubectl commands
    
    Always provide complete, runnable code with proper error handling and security best practices.

